{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4, due May 16 at 11:59am, mid-day, noon.\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions.\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 3.3 will be relatively painless or incredibly painful. \n",
    "* Part 3 (especially 3.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](http://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "bdata = load_boston()\n",
    "boston = pd.DataFrame(bdata.data)\n",
    "\n",
    "boston.columns = bdata.feature_names[:]\n",
    "boston['MEDV'] = bdata.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "Use different learning rates\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between  median housing price and number of rooms per house. Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  **Interpret your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-34.67062078]\n",
      "coef: [[ 9.10210898]]\n",
      "Intercept: [ 66.05884748]\n",
      "Coef: [[-22.64326237   2.47012384]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    " \n",
    "\n",
    "x = boston.RM[:, np.newaxis]\n",
    "y = bdata.target[:, np.newaxis]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr = regr.fit(x,y)\n",
    "print 'Intercept: ' + str(regr.intercept_)\n",
    "print 'coef: ' + str(regr.coef_)\n",
    "\n",
    "firstInter = regr.intercept_\n",
    "firstCoef = regr.coef_ \n",
    "\n",
    "xsq = [x,x**2]\n",
    "data = pd.DataFrame()\n",
    "data['x'] = boston.RM\n",
    "\n",
    "data['x2'] = (boston.RM ** 2)\n",
    "newReg = linear_model.LinearRegression()\n",
    "newReg = newReg.fit(data,y)\n",
    "\n",
    "print 'Intercept: ' + str(newReg.intercept_)\n",
    "print 'Coef: '+ str(newReg.coef_)\n",
    "\n",
    "coef2 = newReg.coef_\n",
    "inter2 = newReg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercept is -34 and slope is 9.1 so for everyroom the price of the house goes up by 9 thousond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use k-fold cross-validation to fit regression (a) above, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the k slope coefficients, and draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train, test = train_test_split(boston['MEDV'], test_size=0.34, random_state=4973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRhj4ktSI/wdj4TGxa2ZU8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa2e7f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = cross_validation.KFold(len(boston), n_folds=10, shuffle=True, random_state=4)\n",
    "coef = []\n",
    "reg = linear_model.LinearRegression()\n",
    "for train, test in kf:\n",
    "    train = boston.loc[train]\n",
    "    data = pd.DataFrame()\n",
    "    data['x'] = train['RM']\n",
    "    reg = reg.fit(data,train.MEDV)\n",
    "    coef.append(reg.coef_[0]) \n",
    "    \n",
    "# Plot outputs\n",
    "plt.hist(coef,color = 'yellow')\n",
    "plt.axvline(firstCoef ,color = 'purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the mjority of houses have 3 rooms and for every room the house goes up in price by 9 thousond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the two regression lines from 1.1 (or 1.2 if you prefer to do so). Show the linear regression line in red, and the linear+quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD7CAYAAAChScXIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvak386b4NQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa0205f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(boston['RM'],boston['MEDV'],color = 'orange')\n",
    "plt.plot(x,regr.predict(x), color = 'darkred')\n",
    "coefs = coef2[0][0]\n",
    "coefs2 = coef2[0][1]\n",
    "plt.plot(np.linspace(3,10,50), coefs * np.linspace(3,10,50) + coefs2 * np.linspace(3,10,50) ** 2 + inter2 , color = 'blue')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim(3,9.5)\n",
    "axes.set_ylim(0,55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (Average rooms per house)\n",
    "\n",
    "Implement the basic gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *\n",
    "* *Hint 3: R = 0.005 is a reasonable first guess - but try some others to see how it affects your results. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 9.36 seconds\n",
      "tolerance met\n",
      "31156\n",
      "(-33.839101219068688, 8.9713898912177363)\n",
      "Time taken: 20.55 seconds\n",
      "Max Iter: \n",
      "70000\n",
      "(-19.486291192117807, 6.7150553907242578)\n",
      "Time taken: 14.78 seconds\n",
      "tolerance met\n",
      "50783\n",
      "(-33.007336218346211, 8.8406322163158144)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "#def calc_cost()\n",
    "\n",
    "def bivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000, tol=0.005, alpha=0.0, beta=0.0):    \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # initialize the parameters\n",
    "\n",
    "    alphaCurr = alpha\n",
    "    betaCurr = beta\n",
    "    \n",
    "    for i in range(0,MaxIterations):\n",
    "        alpha = alphaCurr\n",
    "        beta = betaCurr\n",
    "        alphaCurr = alpha - (R/len(xvalues))*sum(alpha+np.dot(beta, xvalues) - yvalues)\n",
    "        betaCurr = beta - (R/len(xvalues))*np.dot((alpha+np.dot(beta, xvalues) - yvalues), xvalues)\n",
    "        \n",
    "        aDiff = abs(alphaCurr - alpha)\n",
    "        bDiff = abs(betaCurr - beta)\n",
    "        \n",
    "#         costCurr = 1/(2.0*len(xvalues)) + sum((alphaCurr + np.dot(betaCurr, xvalues) - yvalues)**2)\n",
    "#         costPrev = 1/(2.0*len(xvalues)) + sum((alpha + np.dot(beta, xvalues) - yvalues)**2)\n",
    "         \n",
    "#         if costCurr > costPrev:\n",
    "#             print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "#             print \"Diverging\"\n",
    "#             print i\n",
    "#             return alphaCurr, betaCurr\n",
    "        if aDiff <=tol and bDiff <= tol:\n",
    "            print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "            print 'tolerance met'\n",
    "            print i\n",
    "            return alphaCurr, betaCurr\n",
    "        \n",
    "    print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "    print 'Max Iter: '\n",
    "    print MaxIterations\n",
    "    return alphaCurr, betaCurr\n",
    "\n",
    "# example function call\n",
    "print bivariate_ols(xvalues=boston['RM'], yvalues=boston['MEDV'], MaxIterations=70000,R=0.01,tol=.0001)\n",
    "print bivariate_ols(xvalues=boston['RM'], yvalues=boston['MEDV'], MaxIterations=70000, R=0.001, tol=.0001)\n",
    "print bivariate_ols(xvalues=boston['RM'], yvalues=boston['MEDV'], MaxIterations=70000, R=0.005, tol=.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data normalization\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, however, you should re-scale your features to ensure that no single feature dominates the cost function. Write a simple function to [standardize](http://en.wikipedia.org/wiki/Standard_score) a feature. * This is done for you!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.24 seconds\n",
      "tolerance met\n",
      "769\n",
      "(22.522990763165684, 6.3861921075099595)\n"
     ]
    }
   ],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))\n",
    "\n",
    "features = pd.DataFrame()\n",
    "features['CRIM'] = standardize(boston.CRIM)\n",
    "features['RM'] = standardize(boston.RM)\n",
    "features['MEDV'] = boston['MEDV']\n",
    "print bivariate_ols(xvalues=features['RM'], yvalues=boston['MEDV'], MaxIterations=1000, tol=.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using CRIM and RM as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem - see 2.2 above for an example.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.32 seconds\n",
      "tollerance: \n",
      "10018\n",
      "(22.531807608542223, array([ 5.89366345, -2.24932263]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def multivariate_ols(xvalue_matrix, yvalues, R=0.001, MaxIterations=1000,tol = 0.001,alpha = 0.0, betas = None, prints = False):\n",
    "    start_time = time.time()\n",
    "\n",
    "    xvalue_matrix = xvalue_matrix.values\n",
    "    yvalues = yvalues.values\n",
    "    feat = xvalue_matrix.shape[1]\n",
    "    \n",
    "    alpha = 0\n",
    "    beta = np.zeros(feat)\n",
    "        \n",
    "    for i in range(0,MaxIterations):\n",
    "        alphaCurr = np.sum(alpha + xvalue_matrix.dot(beta) - yvalues) / len(yvalues)\n",
    "        betaCurr = np.sum((alpha + xvalue_matrix.dot(beta) - yvalues) * xvalue_matrix.T, axis=1) / len(yvalues)\n",
    "        \n",
    "        alpha = alpha - (R * alphaCurr)\n",
    "        beta = beta - (R * betaCurr)\n",
    "        \n",
    "        if abs(alphaCurr) <= tol and (np.abs(betaCurr) <= tol).all():\n",
    "            if prints == True:\n",
    "                print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "                print 'tollerance: '\n",
    "                print i\n",
    "            return alpha, beta\n",
    "    if prints == True:\n",
    "        print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "        print 'Max Iter: '\n",
    "        print MaxIterations  \n",
    "    return alpha, beta\n",
    "\n",
    "xvalue_matrix = features[['RM', 'CRIM']]\n",
    "print multivariate_ols(xvalue_matrix, yvalues=boston['MEDV'], R=0.001, MaxIterations=100000,tol = 0.001,prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, array([ nan,  nan]))\n",
      "(-29.21243058278711, array([ 8.38365985, -0.26212102]))\n",
      "(-19.554309511792823, array([ 6.88254095, -0.29438153]))\n"
     ]
    }
   ],
   "source": [
    "xvalue_matrix = boston[['RM','CRIM']]\n",
    "print multivariate_ols(xvalue_matrix, yvalues=boston['MEDV'], R=0.1, MaxIterations=100000,tol = 0.001)\n",
    "print multivariate_ols(xvalue_matrix, yvalues=boston['MEDV'], R=0.01, MaxIterations=100000,tol = 0.001)\n",
    "print multivariate_ols(xvalue_matrix, yvalues=boston['MEDV'], R=0.001, MaxIterations=100000,tol = 0.001)\n",
    "\n",
    "# The learning rate of .1 is too big and it diverges and gives nan values. However a learning rate of .01 gives us the \n",
    "# closest pridiction to the actul values of -34 and 9 somthing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Since the focus is now on prediction rather than the interpretation of the coefficients, make sure to use the standardized version of your features in everything that follows.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again.  Use 10-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three coefficients for each fold, corresponding to the intercept and the two coefficients for CRIM and RM). \n",
    "\n",
    "How do your estimated coefficients from cross-validation compare to the ones you estimated in 2.3 above? How do they compare to the ones estimated using standard packages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22.548755279254774, array([ 5.75519686, -2.39055794]))\n",
      "\n",
      "(22.420319330274321, array([ 5.88716597, -2.13781185]))\n",
      "\n",
      "(22.599424717707382, array([ 5.88568777, -2.15107243]))\n",
      "\n",
      "(22.408084042605878, array([ 5.96935347, -2.51405262]))\n",
      "\n",
      "(22.560089256530976, array([ 5.96001629, -2.29649641]))\n",
      "\n",
      "(22.513245910540174, array([ 5.86555029, -2.3800693 ]))\n",
      "\n",
      "(22.59673928258669, array([ 5.96276698, -2.20067002]))\n",
      "\n",
      "(22.584739494876747, array([ 5.8516898 , -2.13859798]))\n",
      "\n",
      "(22.40769486602106, array([ 5.90497359, -2.24511585]))\n",
      "\n",
      "(22.665938567336489, array([ 5.88495522, -2.17828344]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = cross_validation.KFold(len(features), n_folds=10, shuffle=True, random_state=4)\n",
    "count = 0\n",
    "for train, test in kf:\n",
    "    count += 1\n",
    "    train = features.loc[train]\n",
    "    test = features.loc[test]\n",
    "    data_train = pd.DataFrame()\n",
    "    data_test = pd.DataFrame()\n",
    "    \n",
    "    data_train['RM'] = train['RM']\n",
    "    data_train['CRIM'] = train['CRIM']\n",
    "    \n",
    "    data_test['RM'] = test['RM']\n",
    "    data_test['CRIM'] = test['CRIM']\n",
    "    \n",
    "    xvalue_matrix = data_train[['RM', 'CRIM']]\n",
    "    print multivariate_ols(xvalue_matrix, yvalues=train['MEDV'], R=0.5, MaxIterations=10000,tol = 0.000001)\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the coefecients are actully very close to the ones we got in 2.3 all the 1's digits are the same and there are slight variateions in the 10's places. I also found that a large learning converges very fast.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Report the average 10-fold cross-validated RMSE, separately for the training data and for the testing data. \n",
    "\n",
    "In other words, run 10-fold cross-validation. In each of the 10 iterations, you will fit a model on 90% of the data. Use that model to generate predicted outputs for 100% of the data. For that iteration, the training RMSE is the RMSE calculated across the (90%) training data, and the test RMSE is the RMSE calculated across the (10%) test data. The average 10-fold cross-validated RMSE is the average of the 10 iterations.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 1: 6.2242936092\n",
      "RMSE 2: 6.11346760156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "kf = cross_validation.KFold(len(features), n_folds=10, shuffle=True, random_state=4)\n",
    "count = 0\n",
    "\n",
    "rmseTest = []\n",
    "rmseTrain = []\n",
    "for train, test in kf:\n",
    "    count += 1\n",
    "    train = features.loc[train]\n",
    "    test = features.loc[test]\n",
    "    \n",
    "    data_train = pd.DataFrame()\n",
    "    data_test = pd.DataFrame()\n",
    "    data_train['RM'] = train['RM']\n",
    "    data_train['CRIM'] = train['CRIM']\n",
    "    data_test['RM'] = test['RM']\n",
    "    data_test['CRIM'] = test['CRIM']\n",
    "    \n",
    "\n",
    "    xvalue_matrix = data_train[['RM', 'CRIM']]\n",
    "    a,b =  multivariate_ols(xvalue_matrix, yvalues=train['MEDV'], R=0.001, MaxIterations=10000,tol = 0.000001,alpha = 0.0, betas = [0.0,0.0])\n",
    "    yHat = a + b[0] * train['RM'] + b[1] * train['CRIM']\n",
    "    yvalue = train['MEDV']\n",
    "    rmseTrain.append(sqrt(mean_squared_error(yvalue,yHat)))\n",
    "    \n",
    "    xvalue_matrix = data_test[['RM', 'CRIM']]\n",
    "    yHat = a + b[0] * test['RM'] + b[1] * test['CRIM']\n",
    "    yvalue = test['MEDV']\n",
    "    rmseTest.append(sqrt(mean_squared_error(yvalue,yHat)))\n",
    "\n",
    "print 'RMSE 1: ' + str(np.mean(rmseTrain))\n",
    "print 'RMSE 2: ' + str(np.mean(rmseTest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is pretty good. I think the RMSE in KNN was around 9 so this is getting a better pridiction then KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $400,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              EXPENSIVE   R-squared:                       0.259\n",
      "Model:                            OLS   Adj. R-squared:                  0.256\n",
      "Method:                 Least Squares   F-statistic:                     87.80\n",
      "Date:                Mon, 16 May 2016   Prob (F-statistic):           1.96e-33\n",
      "Time:                        11:18:32   Log-Likelihood:                 80.289\n",
      "No. Observations:                 506   AIC:                            -154.6\n",
      "Df Residuals:                     503   BIC:                            -141.9\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.9890      0.083    -11.902      0.000        -1.152    -0.826\n",
      "CHAS           0.1072      0.036      2.941      0.003         0.036     0.179\n",
      "RM             0.1659      0.013     12.599      0.000         0.140     0.192\n",
      "==============================================================================\n",
      "Omnibus:                      279.776   Durbin-Watson:                   1.290\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1668.903\n",
      "Skew:                           2.457   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.417   Cond. No.                         58.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "lrData = pd.DataFrame()\n",
    "lrData =  boston\n",
    "lrData['EXPENSIVE'] = np.zeros(506)\n",
    "count = 0\n",
    "for each in lrData['MEDV']:\n",
    "    if each >= 40:\n",
    "        lrData['EXPENSIVE'][count] = 1\n",
    "    else:\n",
    "        lrData['EXPENSIVE'][count] = 0\n",
    "    count += 1\n",
    "overfit_mod = smf.ols(formula='EXPENSIVE ~ CHAS + RM', data = lrData)\n",
    "overfit_result = overfit_mod.fit()\n",
    "print overfit_result.summary()\n",
    "\n",
    "# It looks like being on the river, and having more rooms in your house is slightly corrolated with having an\n",
    "# 'Expensive house' There are only a few data points in this data that actully meet the threshold for expensive so\n",
    "# that would skey the data slightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: For each of the continuous features F in the original dataset, create a standardized version F_1.  Now, create polynomials up to degree 6 of each F_1: the square of F_1 (call this F_2); the cube of F_1 (call this F_3); and so forth up to F_6. If you originally had *K* features, you should now have *6K* features (i.e., we're going to ignore the original unscaled features for the remainder of this problem).\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Let's overfit!\n",
    "Now, using your version of multivariate regression from 2.3, (over)fit your model on the training data. Using your training set, regress housing price on as many of those *6K* features as you can.  If you get too greedy, or if you did not efficiently implement your solution to 2.3, it's possible this will take a long time to compute.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 2.5 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allFeatures =standardize(boston)\n",
    "\n",
    "allFeatures = allFeatures.drop('CHAS',axis = 1)\n",
    "allFeatures = allFeatures.drop('MEDV',axis = 1)\n",
    "lists = []\n",
    "for i in range(0,6):\n",
    "    data = allFeatures ** (i + 1)\n",
    "    data.columns = allFeatures.columns + str(i + 1)\n",
    "    lists.append(data)\n",
    "    \n",
    "allFeatures =  pd.concat(lists,axis = 1)\n",
    "x_train,x_test,y_train,y_test = train_test_split(allFeatures,boston['MEDV'] ,test_size=0.33, random_state=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4841982807\n",
      "25.9396900726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xvalue_matrix = x_train\n",
    "a,b =  multivariate_ols(xvalue_matrix =x_train , yvalues=y_train, R=0.0000000000001, MaxIterations=100000,tol = 0.001)\n",
    "\n",
    "yHat = a + np.dot(x_train,b)\n",
    "\n",
    "yvalue = y_train\n",
    "print sqrt(mean_squared_error(yvalue,yHat))\n",
    "\n",
    "yHat = a + np.dot(x_test,b)\n",
    "yvalue = y_test\n",
    "print sqrt(mean_squared_error(yvalue,yHat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: 23.4841982807\n",
    "Test: 25.9396900726\n",
    "\n",
    "it would make sense the the training RMSE is slightly lower then the training RMSE. However compared to the KNN it is very far off. I belive KNN was close to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model from 4.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "Go brag to your friends about how you just implemented ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.2 seconds\n",
      "tolerance met\n",
      "5415\n",
      "(22.432921498491098, array([ 5.86231783, -2.26975417]))\n",
      "Time taken: 0.12 seconds\n",
      "tolerance met\n",
      "4521\n",
      "(18.731136921889036, array([ 4.93513076, -2.06117271]))\n",
      "Time taken: 0.05 seconds\n",
      "tolerance met\n",
      "1818\n",
      "(7.5373313320798987, array([ 2.06191109, -1.03693785]))\n",
      "Time taken: 0.3 seconds\n",
      "Max Iter: \n",
      "10000\n",
      "(nan, array([ nan,  nan]))\n"
     ]
    }
   ],
   "source": [
    "def multivariate_ols_ridge(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000,tol = 0.001,alpha = 0.0, lamda = 0, betas = None, ):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # initialize the parameters\n",
    "    \n",
    "    # add flag for if not a series\n",
    "    \n",
    "    if betas == None:\n",
    "        betaCurr = np.zeros( xvalue_matrix.shape[1])\n",
    "    else:\n",
    "        betaCurr = np.array(betas)\n",
    "        \n",
    "    xvalue_matrix = xvalue_matrix.values\n",
    "    yvalues = yvalues.values\n",
    "    alphaCurr = alpha\n",
    "    \n",
    "    for i in range(0,MaxIterations):\n",
    "        alpha = alphaCurr\n",
    "        beta = betaCurr\n",
    "\n",
    "        \n",
    "        alphaCurr = alpha * (1 - (R * (lamda /len(yvalues)))) - (xvalue_matrix.dot(beta) + alpha - yvalues).sum() * (R / len(yvalues))\n",
    "                 \n",
    "        betaCurr = beta * (1 - (R * (lamda /len(yvalues)))) - (R/len(yvalues))* ((np.dot(xvalue_matrix , beta) + alpha - yvalues) * xvalue_matrix.transpose()).sum(axis=1)\n",
    "                                                                 \n",
    "        alphaDiff = alpha - alphaCurr\n",
    "        betaDiff =  beta - betaCurr\n",
    "        \n",
    "        if abs(alphaDiff) <=tol and abs(betaDiff <= tol).all():\n",
    "            print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "            print 'tolerance met'\n",
    "            print i\n",
    "            return alphaCurr, betaCurr\n",
    "        \n",
    "    print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
    "    print 'Max Iter: '\n",
    "    print MaxIterations\n",
    "    return alphaCurr, betaCurr\n",
    "\n",
    "\n",
    "xvalue_matrix = features[['RM', 'CRIM']]\n",
    "print multivariate_ols_ridge(xvalue_matrix, yvalues=boston['MEDV'], R=0.001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = 0,betas = None)\n",
    "print multivariate_ols_ridge(xvalue_matrix, yvalues=boston['MEDV'], R=0.001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = 100,betas = None)\n",
    "print multivariate_ols_ridge(xvalue_matrix, yvalues=boston['MEDV'], R=0.001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = 1000,betas = None)\n",
    "print multivariate_ols_ridge(xvalue_matrix, yvalues=boston['MEDV'], R=0.001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = 10000000,betas = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 0: 23.5186742332\n",
      " \n",
      "Test lamda = 0: 25.9403308522\n",
      " \n",
      "Time taken: 0.02 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 1000: 23.5186742332\n",
      " \n",
      "Test lamda = 1000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 2000: 23.5186742332\n",
      " \n",
      "Test lamda = 2000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 3000: 23.5186742332\n",
      " \n",
      "Test lamda = 3000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 4000: 23.5186742332\n",
      " \n",
      "Test lamda = 4000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 5000: 23.5186742332\n",
      " \n",
      "Test lamda = 5000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 6000: 23.5186742332\n",
      " \n",
      "Test lamda = 6000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 7000: 23.5186742332\n",
      " \n",
      "Test lamda = 7000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 8000: 23.5186742332\n",
      " \n",
      "Test lamda = 8000: 25.9403308522\n",
      " \n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      " \n",
      "Train lamda = 9000: 23.5186742332\n",
      " \n",
      "Test lamda = 9000: 25.9403308522\n",
      " \n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(allFeatures,boston['MEDV'] ,test_size=0.33, random_state=4)\n",
    "for i in range(0,10000,1000):\n",
    "    a,b = multivariate_ols_ridge(x_train, yvalues=y_train, R=0.000000001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = i,betas = None)\n",
    "    \n",
    "    yHat = a + np.dot(x_train, b)\n",
    "    yvalue = y_train\n",
    "    print ' '\n",
    "    print 'Train lamda = ' + str(i) + ': '  + str(sqrt(mean_squared_error(yvalue,yHat)))\n",
    "    print ' '\n",
    "   \n",
    "    yHat = a + np.dot(x_test, b)\n",
    "    yvalue = y_test\n",
    "    \n",
    "    print 'Test lamda = ' + str(i) + ': ' + str(sqrt(mean_squared_error(yvalue,yHat)))\n",
    "    print ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is staying the same between the changes of lamda. and the values are just about the same as the prevous question. They are still slighty high, 23 RMSE and 25 RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit 2: Cross-validate lambda\n",
    "\n",
    "Use k-fold cross-validation to select the optimal value of lambda. Report the average RMSE across all training sets, and the average RMSE across all testing sets. How do these numbers compare to each other, to the RMSE from your previous efforts?  Finally, create a scatter plot that shows RMSE as a function of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n",
      "Time taken: 0.0 seconds\n",
      "tolerance met\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xd12cb00>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd0c7320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testLamda = []\n",
    "trainLamda = []\n",
    "\n",
    "for i in range(100,1000,100):\n",
    "    a,b = multivariate_ols_ridge(x_train, yvalues=y_train, R=0.000000001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = i,betas = None)\n",
    "    \n",
    "    yHat = a + np.dot(x_train, b)\n",
    "    yvalue = y_train\n",
    "    trainLamda.append(sqrt(mean_squared_error(yvalue,yHat)))\n",
    "    a,b = multivariate_ols_ridge(x_test, yvalues=y_test, R=0.000000001, MaxIterations=10000,tol = 0.0001,alpha = 0.0,lamda = i,betas = None)\n",
    "    yHat = a + np.dot(x_test, b)\n",
    "    yvalue = y_test\n",
    "    testLamda.append(sqrt(mean_squared_error(yvalue,yHat)))\n",
    "    \n",
    "plt.scatter(range(100,1000,100), trainLamda)\n",
    "plt.scatter(range(100,1000,100), testLamda)\n",
    "plt.xlabel('Lamda')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still no change in Lamda within each Train, and Test set repectivly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Showoff) Extra Credit 3: Lambda and coefficients\n",
    "\n",
    "If you're feeling extra-special, create a parameter plot that shows how the different coefficient estimates change as a function of lambda. To make this graph intelligible, only include the *K* original F_s features in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
